{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bcf808-969a-4d1b-af6d-c7c2bd500759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import json\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3de137-5caa-4b55-a930-c24c9b7fdc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_generator():\n",
    "    \"\"\" Generates list of urls to scrape from \"\"\"\n",
    "    \n",
    "    # Append new threads here\n",
    "    \n",
    "    thread_list = [\n",
    "    #'https://www.mumsnet.com/talk/am_i_being_unreasonable/4676538-if-you-like-wordle-plusword-is-even-better-thread-4?page=',\n",
    "    #'https://www.mumsnet.com/talk/_chat/4714295-plusword-new-thread-1?page=',\n",
    "    'https://www.mumsnet.com/talk/_chat/4765702-plusword-new-thread-2?page='\n",
    "    ]\n",
    "\n",
    "    url_list=[]\n",
    "\n",
    "    for thread in thread_list:\n",
    "        for page_number in range(1,41):\n",
    "            url = thread + str(page_number)\n",
    "            url_list.append(url)\n",
    "        \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f632e6-0bb5-41e7-a9a0-257aef34ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_text_converter(post):\n",
    "    \"\"\" Converts each post from html to text, then formats and appends meta data to the post body\"\"\"\n",
    "    \n",
    "    # converts to list and removes whitespace\n",
    "    post_text = post.getText().split()\n",
    "    \n",
    "    # separates out meta data and post body\n",
    "    meta_data = post_text[:4]\n",
    "    post_body = post_text[4:]\n",
    "    \n",
    "    # removes fullstop from meta data\n",
    "    meta_data.pop(1)\n",
    "    \n",
    "    # converts whole of post to list\n",
    "    post_body = ' '.join(post_body)\n",
    "    meta_data.append(post_body)\n",
    "    whole_post = meta_data\n",
    "\n",
    "    return whole_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ae3713-09ea-402d-a49e-072d7e3e9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    \n",
    "    \n",
    "    # html class of original post from the thread\n",
    "    original_post_class = 'p-4 pb-1 pt-2.5 lg:py-2.5 mt-2.5 lg:mt-1.5 border-t border-b sm:border sm:rounded ' \\\n",
    "                       'border-mumsnet-forest-border bg-mumsnet-forest dark:bg-mumsnet-forest-dark'\n",
    "\n",
    "    # html class of a normal post from the thread\n",
    "    normal_post_class = 'lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 overflow-x-hidden ' \\\n",
    "                         'bg-white dark:bg-gray-800 border-gray-200'\n",
    "\n",
    "    # html class of a post from the thread creator\n",
    "    original_poster_reply_class = 'lg:py-2.5 pt-2.5 pb-1 p-4 border-t border-b sm:border sm:rounded mt-1.5 ' \\\n",
    "                                  'overflow-x-hidden bg-mumsnet-forest dark:bg-mumsnet-forest-dark ' \\\n",
    "                                  'border-mumsnet-forest-border'\n",
    "    \n",
    "    \n",
    "    all_posts_in_thread = []\n",
    "\n",
    "    #gets web pages\n",
    "    with requests.get(url,  allow_redirects=False) as r:\n",
    "\n",
    "        # returns None if url redirects you to main page\n",
    "        # happens if page number is higher than current page count\n",
    "        if r.status_code != 302:\n",
    "\n",
    "            soup = BeautifulSoup(r.content, features=\"html5lib\")\n",
    "\n",
    "            all_posts_in_url=[]\n",
    "\n",
    "            # selects first page as we need original post\n",
    "            if url[-2:] == '=1':\n",
    "\n",
    "                # Finds original post on first page and splits it into metadata and post text\n",
    "                original_post = soup.find_all('div', class_=original_post_class)\n",
    "                original_post = original_post[0].find_all('div', class_='')\n",
    "                original_post=post_to_text_converter(original_post[2])\n",
    "                all_posts_in_url.append(original_post)\n",
    "\n",
    "\n",
    "            # selects post info and body and drops title and other stuff we dont need\n",
    "            posts = soup.find_all('div', class_=[normal_post_class, original_poster_reply_class])\n",
    "            for post in posts:\n",
    "                whole_post = post_to_text_converter(post)\n",
    "                all_posts_in_url.append(whole_post)\n",
    "        \n",
    "        # returns None if redirected        \n",
    "        else:\n",
    "            return\n",
    "\n",
    "\n",
    "    \n",
    "    return all_posts_in_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edf0a31-2d53-4846-8e4c-7fd1b49ba564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_wrapper(url_list):\n",
    "    \"\"\" Scrapes each page in url list for data and appends results together and creates df from data\"\"\"\n",
    "\n",
    "    all_posts = []\n",
    "    \n",
    "    # Runs scraping script over each item in url list with multithreading\n",
    "    # Then appends \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        all_posts_in_url = executor.map(scraper, url_list)\n",
    "        for page in all_posts_in_url:\n",
    "            all_posts.append(page)\n",
    "            \n",
    "    # Removes any None values from exiting function early\n",
    "    all_posts= [x for x in all_posts if x is not None]\n",
    "    \n",
    "    # Drops list level\n",
    "    all_posts = [val for sublist in all_posts for val in sublist]\n",
    "\n",
    "    df = pd.DataFrame(all_posts, columns=['user', 'date', 'time', 'text'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf017846-1cbf-4a6a-a56c-afef6443fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    \"\"\" Cleans up data ready for adding to db\"\"\"\n",
    "    \n",
    "    \n",
    "    df['date'] = df['date'].str.replace('Yesterday', dt.datetime.strftime((dt.datetime.today() - dt.timedelta(days=1)), '%d/%m/%Y'))\n",
    "    df['date'] = df['date'].str.replace('Today', dt.datetime.strftime(dt.datetime.today(), '%d/%m/%Y'))\n",
    "    df['load_ts'] = df['date'] + ' ' + (df['time']+':00')\n",
    "    df['load_ts'] = df['load_ts'] + '.000'\n",
    "    df['load_ts'] = pd.to_datetime(df['load_ts'], format='%d/%m/%Y %H:%M:%S.%f')\n",
    "    df = df.sort_values(by=['load_ts'])\n",
    "    \n",
    "    df['text'] =df['text'].str.extract(r'(\\d*\\d:\\d\\d)')\n",
    "    df = df.dropna(subset='text')\n",
    "    df = df.copy()\n",
    "    df['text'] =df['text'].str.replace(r'(^\\d:\\d\\d)', r'0\\1', regex=True)\n",
    "    df['text'] = '00:' + df['text']\n",
    "    \n",
    "    df = df.copy()\n",
    "    df= df.drop_duplicates(subset=['user', 'date'])\n",
    "    df = df.drop(columns=['date', 'time'])\n",
    "    df = df.rename(columns={'text' : 'time'})\n",
    "    df = df[['load_ts', 'time', 'user']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f39d4bf-c17f-4d15-b3a9-6d394ec34465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db(write=False):\n",
    "    if write:\n",
    "        connection_string = \"admin_connection_string\"\n",
    "\n",
    "    else:\n",
    "        connection_string = \"connection_string\"\n",
    "\n",
    "    try:\n",
    "        with open(\"../local/pass.json\") as file:\n",
    "            file = json.loads(file.read())\n",
    "            connection_string = file.get(connection_string)\n",
    "            client = pymongo.MongoClient(\n",
    "                connection_string)\n",
    "            db = client[\"PlusWord\"]\n",
    "            return db\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497d162d-37c0-4699-8ace-172ff4e32974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mums_data_import():\n",
    "    \"\"\"Connects to database and creates dataframe containing all columns. Drops unneeded columns and sets timestamp\n",
    "     datatype. Creates submission time from timestamp and converts both submission time and completion time to time\n",
    "     deltas represented as plottable numbers. Finally, drops submission time column as no longer needed\"\"\"\n",
    "\n",
    "    # Connects to db and gets collection\n",
    "    db = get_db()\n",
    "    collection = db['Mumsnet_Times']\n",
    "    df = pd.DataFrame(list(collection.find({})))\n",
    "    df = df[['load_ts', 'time', 'user']]\n",
    "    df['load_ts'] = pd.to_datetime(df['load_ts'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    df = df.sort_values(by=['load_ts'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e793a3de-c10c-418a-83bf-b2f9912f94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_old_rows(df):\n",
    "    \"\"\" Gets data from database and filters out rows that are already in the database\"\"\"\n",
    "\n",
    "    df_mums = mums_data_import()\n",
    "    df = df.set_index(['load_ts', 'user'])\n",
    "    df_mums = df_mums.set_index(['load_ts', 'user'])\n",
    "    df = df[~df.index.isin(df_mums.index)].reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ea8996-83bd-444b-9b7b-a76b49183a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_export(df):\n",
    "    \"\"\" If dataframe isn't empty then rows are written to the database\"\"\"\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            db = get_db(write=True)\n",
    "            collection = db['Mumsnet_Times']\n",
    "            collection.insert_many(df.to_dict('records'))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eba592a-b26f-4cdc-87ad-9d8f23b8dd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_list= url_generator()\n",
    "df_raw = multithread_wrapper(url_list)\n",
    "df_clean = cleaning(df_raw)\n",
    "df = filter_out_old_rows(df_clean)\n",
    "data_export(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094dd576-064e-4b42-b5e2-2019dd8f1707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_ts</th>\n",
       "      <th>user</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [load_ts, user, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e99e2d-8726-4052-9fe3-a5d34c42c4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
